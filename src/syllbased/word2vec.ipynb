{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-21 12:33:29,259 : INFO : collecting all words and their counts\n",
      "2017-11-21 12:33:29,262 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-21 12:34:24,923 : INFO : collected 6470689 word types from a corpus of 66747827 raw words and 876 sentences\n",
      "2017-11-21 12:34:24,925 : INFO : Loading a fresh vocabulary\n",
      "2017-11-21 12:34:29,488 : INFO : min_count=10 retains 198132 unique words (3% of original 6470689, drops 6272557)\n",
      "2017-11-21 12:34:29,489 : INFO : min_count=10 leaves 58442679 word corpus (87% of original 66747827, drops 8305148)\n",
      "2017-11-21 12:34:30,157 : INFO : deleting the raw counts dictionary of 6470689 items\n",
      "2017-11-21 12:34:30,542 : INFO : sample=0.001 downsamples 33 most-common words\n",
      "2017-11-21 12:34:30,543 : INFO : downsampling leaves estimated 47399105 word corpus (81.1% of prior 58442679)\n",
      "2017-11-21 12:34:30,545 : INFO : estimated required memory for 198132 words and 100 dimensions: 257571600 bytes\n",
      "2017-11-21 12:34:31,545 : INFO : resetting layer weights\n",
      "2017-11-21 12:34:34,606 : INFO : training model with 3 workers on 198132 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-21 12:34:35,629 : INFO : PROGRESS: at 2.08% examples, 411580 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:36,653 : INFO : PROGRESS: at 2.97% examples, 396213 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:37,663 : INFO : PROGRESS: at 3.93% examples, 402699 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:38,666 : INFO : PROGRESS: at 4.89% examples, 406679 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:34:39,680 : INFO : PROGRESS: at 5.84% examples, 408150 words/s, in_qsize 1, out_qsize 1\n",
      "2017-11-21 12:34:40,691 : INFO : PROGRESS: at 6.83% examples, 411000 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:41,702 : INFO : PROGRESS: at 7.60% examples, 400355 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:42,723 : INFO : PROGRESS: at 8.58% examples, 402978 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:43,738 : INFO : PROGRESS: at 9.52% examples, 403115 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:44,747 : INFO : PROGRESS: at 10.30% examples, 396530 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:45,766 : INFO : PROGRESS: at 11.07% examples, 390795 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:46,785 : INFO : PROGRESS: at 11.74% examples, 381901 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:47,804 : INFO : PROGRESS: at 12.51% examples, 378172 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:48,807 : INFO : PROGRESS: at 13.29% examples, 375412 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:34:49,836 : INFO : PROGRESS: at 14.11% examples, 373677 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:34:50,855 : INFO : PROGRESS: at 14.93% examples, 372406 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:51,856 : INFO : PROGRESS: at 15.62% examples, 368173 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:34:52,863 : INFO : PROGRESS: at 16.42% examples, 367049 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:53,865 : INFO : PROGRESS: at 17.21% examples, 366114 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:54,877 : INFO : PROGRESS: at 17.99% examples, 364620 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:55,906 : INFO : PROGRESS: at 18.79% examples, 363434 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:56,914 : INFO : PROGRESS: at 19.50% examples, 360903 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:34:57,922 : INFO : PROGRESS: at 21.85% examples, 362891 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:34:58,933 : INFO : PROGRESS: at 22.83% examples, 365485 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:34:59,936 : INFO : PROGRESS: at 23.77% examples, 367205 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:00,972 : INFO : PROGRESS: at 24.77% examples, 369466 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:01,998 : INFO : PROGRESS: at 25.71% examples, 370591 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:03,030 : INFO : PROGRESS: at 26.69% examples, 372263 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:04,044 : INFO : PROGRESS: at 27.53% examples, 372020 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:05,055 : INFO : PROGRESS: at 28.49% examples, 373452 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:06,084 : INFO : PROGRESS: at 29.47% examples, 374908 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:07,092 : INFO : PROGRESS: at 30.09% examples, 371584 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:08,111 : INFO : PROGRESS: at 30.89% examples, 370739 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:09,128 : INFO : PROGRESS: at 31.69% examples, 369954 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:10,150 : INFO : PROGRESS: at 32.47% examples, 368883 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:11,165 : INFO : PROGRESS: at 33.24% examples, 367934 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:12,203 : INFO : PROGRESS: at 34.04% examples, 367091 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:35:13,230 : INFO : PROGRESS: at 34.70% examples, 364841 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:35:14,235 : INFO : PROGRESS: at 35.32% examples, 362401 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:15,247 : INFO : PROGRESS: at 36.10% examples, 361744 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:16,257 : INFO : PROGRESS: at 36.80% examples, 360413 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:17,283 : INFO : PROGRESS: at 37.60% examples, 359945 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:18,288 : INFO : PROGRESS: at 38.38% examples, 359454 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:19,315 : INFO : PROGRESS: at 39.18% examples, 359020 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:20,328 : INFO : PROGRESS: at 41.42% examples, 358942 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:21,341 : INFO : PROGRESS: at 42.42% examples, 360580 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:22,351 : INFO : PROGRESS: at 43.38% examples, 361747 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:35:23,368 : INFO : PROGRESS: at 44.32% examples, 362615 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:24,387 : INFO : PROGRESS: at 45.30% examples, 363831 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:25,389 : INFO : PROGRESS: at 46.26% examples, 364919 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:26,406 : INFO : PROGRESS: at 47.17% examples, 365479 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:27,414 : INFO : PROGRESS: at 48.08% examples, 366075 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:28,443 : INFO : PROGRESS: at 49.06% examples, 367069 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:29,460 : INFO : PROGRESS: at 49.95% examples, 367369 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:30,479 : INFO : PROGRESS: at 50.75% examples, 366936 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:31,493 : INFO : PROGRESS: at 51.55% examples, 366547 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:32,518 : INFO : PROGRESS: at 52.33% examples, 365930 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:33,531 : INFO : PROGRESS: at 53.13% examples, 365580 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:34,566 : INFO : PROGRESS: at 53.93% examples, 365105 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:35,567 : INFO : PROGRESS: at 54.66% examples, 364360 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:36,580 : INFO : PROGRESS: at 55.37% examples, 363410 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:37,593 : INFO : PROGRESS: at 56.00% examples, 362008 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:35:38,603 : INFO : PROGRESS: at 56.80% examples, 361767 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:39,628 : INFO : PROGRESS: at 57.60% examples, 361445 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:40,644 : INFO : PROGRESS: at 58.40% examples, 361186 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:41,667 : INFO : PROGRESS: at 59.18% examples, 360743 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:42,673 : INFO : PROGRESS: at 61.42% examples, 360704 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:43,683 : INFO : PROGRESS: at 62.40% examples, 361654 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:44,705 : INFO : PROGRESS: at 63.33% examples, 362233 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:45,721 : INFO : PROGRESS: at 64.29% examples, 362962 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:46,736 : INFO : PROGRESS: at 65.11% examples, 362844 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:47,753 : INFO : PROGRESS: at 66.05% examples, 363408 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:48,774 : INFO : PROGRESS: at 67.03% examples, 364201 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:49,794 : INFO : PROGRESS: at 68.01% examples, 364979 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:50,802 : INFO : PROGRESS: at 69.00% examples, 365794 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:51,824 : INFO : PROGRESS: at 69.89% examples, 366006 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:52,841 : INFO : PROGRESS: at 70.68% examples, 365719 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:53,862 : INFO : PROGRESS: at 71.48% examples, 365426 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:54,881 : INFO : PROGRESS: at 72.26% examples, 365022 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:55,904 : INFO : PROGRESS: at 73.06% examples, 364734 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:56,926 : INFO : PROGRESS: at 73.79% examples, 364095 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:57,941 : INFO : PROGRESS: at 74.59% examples, 363858 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:58,957 : INFO : PROGRESS: at 75.39% examples, 363626 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:35:59,970 : INFO : PROGRESS: at 76.19% examples, 363410 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:00,984 : INFO : PROGRESS: at 76.99% examples, 363195 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:01,986 : INFO : PROGRESS: at 77.76% examples, 362924 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:03,006 : INFO : PROGRESS: at 78.56% examples, 362696 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:04,029 : INFO : PROGRESS: at 79.36% examples, 362459 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:05,041 : INFO : PROGRESS: at 81.67% examples, 362717 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:06,068 : INFO : PROGRESS: at 82.65% examples, 363347 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:07,078 : INFO : PROGRESS: at 83.54% examples, 363597 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:08,094 : INFO : PROGRESS: at 84.27% examples, 363065 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:09,109 : INFO : PROGRESS: at 85.25% examples, 363716 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:10,119 : INFO : PROGRESS: at 86.23% examples, 364374 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:11,137 : INFO : PROGRESS: at 87.21% examples, 364985 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:12,155 : INFO : PROGRESS: at 88.20% examples, 365585 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:13,157 : INFO : PROGRESS: at 89.18% examples, 366231 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:14,165 : INFO : PROGRESS: at 89.93% examples, 365840 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:15,180 : INFO : PROGRESS: at 90.64% examples, 365227 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:16,210 : INFO : PROGRESS: at 91.44% examples, 364969 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:17,216 : INFO : PROGRESS: at 92.19% examples, 364607 words/s, in_qsize 1, out_qsize 0\n",
      "2017-11-21 12:36:18,240 : INFO : PROGRESS: at 92.97% examples, 364285 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:19,258 : INFO : PROGRESS: at 93.77% examples, 364086 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:20,277 : INFO : PROGRESS: at 94.52% examples, 363700 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:21,319 : INFO : PROGRESS: at 95.18% examples, 362867 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:22,331 : INFO : PROGRESS: at 95.98% examples, 362706 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:23,335 : INFO : PROGRESS: at 96.76% examples, 362484 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:24,358 : INFO : PROGRESS: at 97.56% examples, 362295 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:25,367 : INFO : PROGRESS: at 98.26% examples, 361794 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:26,386 : INFO : PROGRESS: at 99.02% examples, 361447 words/s, in_qsize 0, out_qsize 0\n",
      "2017-11-21 12:36:27,283 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-21 12:36:27,305 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-21 12:36:27,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-21 12:36:27,322 : INFO : training on 333739135 raw words (40700567 effective words) took 112.7s, 361107 effective words/s\n",
      "2017-11-21 12:36:27,334 : INFO : saving Word2Vec object under mymodel, separately None\n",
      "2017-11-21 12:36:27,336 : INFO : storing np array 'syn0' to mymodel.wv.syn0.npy\n",
      "2017-11-21 12:36:28,501 : INFO : not storing attribute syn0norm\n",
      "2017-11-21 12:36:28,504 : INFO : storing np array 'syn1neg' to mymodel.syn1neg.npy\n",
      "2017-11-21 12:36:29,228 : INFO : not storing attribute cum_table\n",
      "2017-11-21 12:36:30,515 : INFO : saved mymodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "#Adapted from the tutorial: https://rare-technologies.com/word2vec-tutorial/\n",
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "import os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "                \n",
    "path = '../files/syllabSQL' #path for the sql file to be read in \n",
    "\n",
    "sentences = MySentences(path) # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences, min_count=10) #to filter out words with count that is at least 10\n",
    "\n",
    "model.save('mymodel')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other genism and word2vec functions/code that may be useful\n",
    "\n",
    "# Following code can be used to load model\n",
    "#model = Word2Vec.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "#model = Word2Vec.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)\n",
    "#model = gensim.models.Word2Vec.load('/tmp/mymodel')\n",
    "\n",
    "#Can be used to train the model\n",
    "#model.train(more_sentences)\n",
    "\n",
    "\n",
    "#model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet\n",
    "#model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator\n",
    "#model.train(other_sentences)  # can be a non-repeatable, 1-pass generator\n",
    "\n",
    "#model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-21 14:16:22,007 : INFO : loading Word2Vec object from mymodel\n",
      "2017-11-21 14:16:23,612 : INFO : loading wv recursively from mymodel.wv.* with mmap=None\n",
      "2017-11-21 14:16:23,612 : INFO : loading syn0 from mymodel.wv.syn0.npy with mmap=None\n",
      "2017-11-21 14:16:23,723 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-21 14:16:23,724 : INFO : loading syn1neg from mymodel.syn1neg.npy with mmap=None\n",
      "2017-11-21 14:16:23,831 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-21 14:16:23,832 : INFO : loaded mymodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761696956832\n",
      "[('fierce', 0.8535844683647156)]\n",
      "[('analog', 0.8831367492675781), ('automated', 0.8725520372390747), ('optimal', 0.8725296258926392), ('plotting', 0.870543360710144), ('three-dimensional', 0.867868185043335)]\n",
      "[('biology', 0.7917934060096741), ('mathematics', 0.779987633228302), ('statistics', 0.7579514980316162), ('engineering', 0.7559429407119751), ('technology', 0.7493206262588501)]\n",
      "cereal\n"
     ]
    }
   ],
   "source": [
    "#tests for word2vec model \n",
    "new_model = gensim.models.Word2Vec.load('mymodel')\n",
    "print(model.similarity('programming', 'computer'))\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "print(model.most_similar('algorithm', topn=5)) #prints out top 5 similar words [useful for study guides]\n",
    "print(model.most_similar(positive=['computer', 'science'], negative=[], topn=5)) #concatenates and prints out top 5 similar words [useful for study guides]\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.140861573189\n",
      "0.365727256496\n",
      "0.417392941035\n",
      "0.414461588031\n",
      "[('fierce', 0.8535844683647156)]\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "\n",
    "#More testing with computer science related words\n",
    "new_model = gensim.models.Word2Vec.load('mymodel')\n",
    "print(new_model.similarity('computer', 'equals'))\n",
    "print(new_model.similarity('computer', 'CS101'))\n",
    "print(new_model.similarity('computer', 'variable'))\n",
    "print(new_model.similarity('computer', 'syntax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample code for TFIDF\n",
    "\n",
    "#documents = []\n",
    "\n",
    "#with open('closed_caption.txt', 'r') as f:\n",
    "#    documents = [line.strip() for line in f]\n",
    "#vectorizer = TfidfVectorizer(stop_words='english')\n",
    "#documents = documents[:300]\n",
    "#X = vectorizer.fit_transform(documents)\n",
    "\n",
    "#getting top n terms\n",
    "#feature_array = np.array(tfidf.get_feature_names())\n",
    "#tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "#n = 3\n",
    "#top_n = feature_array[tfidf_sorting][:n]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
